{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training_Model_Woojin.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPWMWMZSIQf0wli0hvTlrn3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hyunul/MLDL/blob/main/Training_Model_Woojin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 선형회귀 (Linear Regression)\n",
        "- 정의 : 데이터를 놓고 그걸 가장 잘 설명할 수 있는 선을 찾아 분석하는 방법\n",
        "- 목표 : 모든 데이터로부터 나타나는 오차의 평균을 최소화할 수 있는 최적의 기울기와 절편 찾기\n",
        "\n"
      ],
      "metadata": {
        "id": "UJ7CcSAEeXan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "x = 2 * np.random.rand(100,1) # [0, 1) 범위에서 균일한 분포 100 X 1 array \n",
        "y = 4 + 3*x + np.random.randn(100,1) # normal distribution(mu=0,var=1)분포 100 X 1 array\n",
        "plt.scatter(x,y)\n",
        "\n",
        "x_b = np.c_[np.ones((100,1)),x] # 모든 샘플에 index 0번에 1을 추가\n",
        "\n",
        "# np.linalg.inv는 넘파이 선형대수 모듈(linalg)의 inv(역함수)\n",
        "# .dot은 행렬 곱셈\n",
        "theta_best = np.linalg.inv(x_b.T.dot(x_b)).dot(x_b.T).dot(y)\n",
        "\n",
        "x_new = np.array([[0],[2]])\n",
        "x_new_b = np.c_[np.ones((2,1)),x_new]\n",
        "prediction = x_new_b.dot(theta_best)\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(x,y)\n",
        "\n",
        "plt.plot(x_new,prediction,\"r-\")\n",
        "plt.plot(x,y,\"b.\")\n",
        "plt.axis([0,2,0,15]) # x축 범위 0~2, y축 범위 0~15 \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bpkVw6OLhtOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 경사 하강법 (Gradient Descent)\n",
        "- 1차 근삿값 발견을 위한 최적화 알고리즘\n",
        "- 함수의 기울기(경사)를 구하고 경사의 반대 방향으로 계속 이동시켜 극값에 이를 때까지 반복시키는 것.\n",
        "- 추적 추정치를 수식으로 계산하기 어렵거나 변수가 너무 많거나 training set이 너무 클 경우 주로 사용함."
      ],
      "metadata": {
        "id": "g_Kh07tZiosu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def make_linear(w=0.5, b=0.8, size=50, noise=1.0):\n",
        "    x = np.random.rand(size)\n",
        "    y = w * x + b\n",
        "    noise = np.random.uniform(-abs(noise), abs(noise), size=y.shape)\n",
        "    yy = y + noise\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.plot(x, y, color='r', label=f'y = {w}*x + {b}')\n",
        "    plt.scatter(x, yy, label='data')\n",
        "    plt.legend(fontsize=20)\n",
        "    plt.show()\n",
        "    print(f'w: {w}, b: {b}')\n",
        "    return x, yy\n",
        "\n",
        "x, y = make_linear(w=0.3, b=0.5, size=100, noise=0.01)"
      ],
      "metadata": {
        "id": "N7gyKz4YjGRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 배치 경사 하강법 (Batch Gradient Decent)\n",
        "- 파라미터가 2개 이상인 경우를 주로 의미\n",
        "- 매 단계에서 training set 전체를 사용하여 그래디언트를 계산하고 파라마터를 업데이트함."
      ],
      "metadata": {
        "id": "tLBFNjp4phav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = 1.5 * np.random.rand(100, 1)\n",
        "y = 6 - 2 * X +  np.random.randn(100, 1)\n",
        "# y = 6 - 2*x\n",
        "\n",
        "X_b = np.c_[np.ones((100, 1)), X]"
      ],
      "metadata": {
        "id": "WC6fFfnupJ76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eta = 0.01 # 내려가는 스탭의 크기\n",
        "n_iter = 10000 # 10000번 반복\n",
        "m = 100 # sample 수\n",
        "B_hat_BGD = np.random.randn(2,1) # B_hat 초기값\n",
        "\n",
        "for iteration in range(n_iter):\n",
        "    gradients = 2/m * X_b.T.dot(X_b.dot(B_hat_BGD) - y)\n",
        "    B_hat_BGD = B_hat_BGD - eta * gradients"
      ],
      "metadata": {
        "id": "_W_5LK8kx3Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B_hat = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)  # linalg : 선형대수 모듈"
      ],
      "metadata": {
        "id": "lxYx7Zlu0Ti-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B_path_BGD = []\n",
        "\n",
        "def plot_gradient_descent(B_hat, eta, B_path=None):\n",
        "    m = 100\n",
        "    plt.plot(X, y, \"b.\")\n",
        "    n_iter = 10000\n",
        "    for iter in range(n_iter):\n",
        "        if iter%1000 == 0:\n",
        "            X_new = np.array([[0], [1.5]])  # X의 범위 [0, 1.5]\n",
        "            X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
        "            y_predict = X_new_b.dot(B_hat)\n",
        "            \n",
        "            style = \"r-\" if iter > 0 else \"g--\"\n",
        "            plt.plot(X_new, y_predict, style)\n",
        "        gradients = 2/m * X_b.T.dot(X_b.dot(B_hat) - y)\n",
        "        B_hat = B_hat - eta * gradients\n",
        "        if B_path is not None:\n",
        "            B_path.append(B_hat)\n",
        "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "    plt.axis([0, 1.5, 0, 10])\n",
        "    plt.title(r\"$\\eta = {}$\".format(eta), fontsize=16)"
      ],
      "metadata": {
        "id": "HnajQfxs0Vzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(42)\n",
        "\n",
        "B_hat_BGD = 2*np.random.randn(2,1)  # random initialization\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(131); plot_gradient_descent(B_hat_BGD, eta=0.0)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.subplot(132); plot_gradient_descent(B_hat_BGD, eta=0.01, B_path=B_path_BGD)\n",
        "plt.subplot(133); plot_gradient_descent(B_hat_BGD, eta=0.001, B_path=B_path_BGD)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gh4hnKia0XQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이는 하이퍼 파라미터를 잘 조절해서 정규방정식으로 구한 것과 마찬가지로 최소 분산 예측치를 구하는 것인데, 왜 경사하강법을 사용하는가?\n",
        "\n",
        "=> 계산복잡도때문임. 정규방정식으로 푼 최소 분산 예측치의 계산복잡도는 변수 개수가 늘어날수록 시간이 세제곱배로 증가하기 때문임.\n"
      ],
      "metadata": {
        "id": "p3zZNC3t0-a9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 확률적 경사 하강법 (Stochastic Gradient Descent)\n",
        "- 장점 : 배치 경사 하강법에 비해 빠르고 비용함수가 불규칙할 경우 알고리즘이 지역 최솟값을 건너뛸 가능성이 높음, 하나의 샘플에 대한 그래디언트만 계산, 즉 데이터가 매우 클 때 유용함\n",
        "- 단점 : 배치 경사 하강법에 비해 불안정함, 일정하게 비용함수가 감소하는 것이 아니라 요동치기 때문에 평균적으로 감소 배치 경사 하강법에 비해 전역 최솟값에 다다르지 못함, 하이퍼 파라미터 증가\n",
        "\n",
        "=> 단점 극복 방안 : 학습률을 점진적으로 줄이는 학습 스케쥴 설정."
      ],
      "metadata": {
        "id": "NNsfXUBN1bHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 50\n",
        "t0, t1 = 5, 50  # 학습 스케줄 하이퍼 파라미터 learning schedule hyperparameters\n",
        "eta = 0.01\n",
        "\n",
        "B_hat_SGD = np.random.randn(2,1)  # 초기화\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(m):\n",
        "        stoch_index = np.random.randint(m)\n",
        "        xi = X_b[stoch_index:stoch_index+1]\n",
        "        yi = y[stoch_index:stoch_index+1]\n",
        "        \n",
        "        gradients = 2 * xi.T.dot(xi.dot(B_hat_SGD) - yi)\n",
        "        B_hat_SGD = B_hat_SGD - eta * gradients   "
      ],
      "metadata": {
        "id": "woDHmzf30YPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 미니 배치 경사 하강법 (Mini-Batch Gradient Descent)\n",
        " - 랜덤한 파트가 없으나 이와 비슷하게 모든 데이터를 매번 사용하지 않음.\n",
        " - 장점 : 배치 경사 하강법에 비해 빠름, 확률적 경사 하강법과 달리 랜덤 파트도 없어서 더욱 빠름, 미니배치가 어느정도 크면 확률적 경사 하강법에 비해 전역 최솟값에 더 가까이 도달\n",
        " - 단점 : 확률적 경사 하강법에 비해 지역 최솟값에서 빠져나오기는 더 힘들 수 있음, 마찬가지로 하이퍼 파라미터 증가"
      ],
      "metadata": {
        "id": "D8LBUYL22GbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epoch = 50\n",
        "minibatch_size = 20\n",
        "\n",
        "np.random.seed(42)\n",
        "B_hat_MGD = np.random.randn(2,1)  # 무작위 초기화\n",
        "\n",
        "# learning schedule을 추가\n",
        "t0, t1 = 200, 1000\n",
        "def learning_schedule(t):\n",
        "    return t0 / (t + t1)\n",
        "\n",
        "t = 0\n",
        "for epoch in range(n_epoch):\n",
        "    shuffled_indices = np.random.permutation(m)\n",
        "    X_b_shuffled = X_b[shuffled_indices]\n",
        "    y_shuffled = y[shuffled_indices]\n",
        "    \n",
        "    for i in range(0, m, minibatch_size):\n",
        "        t += 1\n",
        "        xi = X_b_shuffled[i:i+minibatch_size]\n",
        "        yi = y_shuffled[i:i+minibatch_size]\n",
        "        gradients = 2/minibatch_size * xi.T.dot(xi.dot(B_hat_MGD) - yi)\n",
        "        eta = learning_schedule(t)\n",
        "        B_hat_MGD = B_hat_MGD - eta * gradients"
      ],
      "metadata": {
        "id": "S1rLHi7E2GNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **정리**\n",
        " - 확률적 경사 하강법과 미니 배치 경사 하강법을 비교해보면, 확률적 경사 하강법은 배치 크기가 1이나 미니 경사 하강법에서는 배치 크기가 >1로 조금 더 크게 설정하는 것이 일반적임.\n",
        " - 미니 배치에서 한 반복에 있어서 한번도 사용하지 않은 데이터는 없으나, 확률 경사법에서는 한번도 사용하지 않은 데이터가 있을 수도 있다는 것이 큰 차이점.\n",
        "\n",
        "출처 : https://givitallugot.github.io/articles/2020-07/Python-gradient-descent-2"
      ],
      "metadata": {
        "id": "OhOwE8sR2xaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "P8MHm8k94B5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 다항 회귀 (Polynomial Regression)\n",
        " - 비선형 데이터를 학습하기 위해 선형 모델을 사용하는 기법"
      ],
      "metadata": {
        "id": "OHXZdWEk4Hak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "m = 100\n",
        "X = 6 * np.random.rand(m,1) - 3\n",
        "y = 0.5 * X**2 + X + 2 + np.random.randn(m,1)  # 약간의 노이즈 포함\n",
        "\n",
        "plt.plot(X,y,\"b.\")\n",
        "plt.ylabel(\"Y\", fontsize=15,rotation=0)\n",
        "plt.xlabel(\"X\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IQ1j8Cn12qI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "#degree = 2 : 2차 다항 생성\n",
        "#include_bias = False : default가 True인데 True이면 편향을 위한 변수인 1이 추가된다.\n",
        "#즉, 각 변수값들(X)을 제곱하여 새로운 변수를 만들어 주는 역할을 함.\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "print(X[0])\n",
        "print(X_poly[0])"
      ],
      "metadata": {
        "id": "uUVVc4-n40Nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_poly, y)\n",
        "lin_reg.intercept_ , lin_reg.coef_"
      ],
      "metadata": {
        "id": "movT6vpp427y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
        "X_new_poly = poly_features.transform(X_new)\n",
        "y_new = lin_reg.predict(X_new_poly)\n",
        "plt.plot(X, y, \"b.\")\n",
        "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.legend(loc=\"upper left\", fontsize=14)\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F83-c4jh5UcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#능형 회귀 (Ridge Regressiong)\n",
        "- 흔히 알고 있는 선형모델의 오차를 최소화하는 계수를 찾기 위한 최고제곱법과 매우 유사하지만 \"각 계수의 제곱을 더한 값\"을 식에 포함하여 계수의 크기도 함께 최소화하도록 만들었다는 차이를 가지고 있다.\n",
        "- 장점 : 오차를 최소화하는 함수에 페널티를 줌으로써 보다 부드럽게 계수를 선택할 수 있음.\n",
        "- 사용하는 이유 : 선형 회귀의 계수 자체가 크면 페널티를 주는 수식을 추가한 것이 능형 회귀이다. 능형 회귀는 기본 선형 모델을 정규화하여 좀 더 좋은 성능을 내기 위해 시도해볼 수 있는 아주 기본적인 기법이다."
      ],
      "metadata": {
        "id": "9zJ3J8uQ6EX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lasso\n",
        " - 최소제곱법과 매우 유사하나, \"각 계수 절댓값의 합\"을 수식에 포함하여 계수의 크기도 함께 최소화하도록 만들었다는 차이를 가지고 있음.\n",
        " - Lasso를 사용하면 자연스럽게 subset selection 효과가 나타난다. "
      ],
      "metadata": {
        "id": "PdGrGoHYhf1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Elastic Net\n",
        " - 큰 데이터 셋에서 잘 작동한다. Ridge 회귀와 Lasso의 장점을 모두 가지고 있어 변수의 수도 줄이고 variance도 줄이고 싶을 때 사용."
      ],
      "metadata": {
        "id": "ktUZXpKlnCxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression\n",
        " - 회귀를 사용하여 데이터가 어떤 범주에 속할 확률을 0과 1 사이의 값으로 예측하고 그 확률에 따라 가능성이 더 높은 범주에 속하는 것으로 분류해주는 지도 학습 알고리즘"
      ],
      "metadata": {
        "id": "H0zjWLOqn6lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Tr27q4lCei5z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}